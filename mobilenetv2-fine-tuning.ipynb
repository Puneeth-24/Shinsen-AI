{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3173719,"sourceType":"datasetVersion","datasetId":952827},{"sourceId":13794170,"sourceType":"datasetVersion","datasetId":8782138}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download dataset","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"kritikseth/fruit-and-vegetable-image-recognition\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T16:04:33.789248Z","iopub.execute_input":"2025-11-19T16:04:33.789414Z","iopub.status.idle":"2025-11-19T16:05:26.648075Z","shell.execute_reply.started":"2025-11-19T16:04:33.789399Z","shell.execute_reply":"2025-11-19T16:05:26.647362Z"}},"outputs":[{"name":"stdout","text":"Mounting files to /kaggle/input/fruit-and-vegetable-image-recognition...\nPath to dataset files: /kaggle/input/fruit-and-vegetable-image-recognition\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Fine tuning model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import MobileNetV2\nimport os\n\n# -----------------------------\n# 1. Basic config\n# -----------------------------\nDATA_DIR = \"/kaggle/input/fruit-and-vegetable-image-recognition\"          # root folder\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nVAL_DIR   = os.path.join(DATA_DIR, \"validation\")\nTEST_DIR  = os.path.join(DATA_DIR, \"test\")   # optional, for final eval\n\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\nSEED = 42\n\n# -----------------------------\n# 2. Load datasets\n# -----------------------------\ntrain_ds = keras.utils.image_dataset_from_directory(\n    TRAIN_DIR,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    image_size=IMG_SIZE,\n    shuffle=True,\n    seed=SEED,\n)\n\nval_ds = keras.utils.image_dataset_from_directory(\n    VAL_DIR,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    image_size=IMG_SIZE,\n    shuffle=False,\n)\n\ntest_ds = keras.utils.image_dataset_from_directory(\n    TEST_DIR,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    image_size=IMG_SIZE,\n    shuffle=False,\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(\"Classes:\", class_names)\nprint(\"Num classes:\", num_classes)\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.prefetch(AUTOTUNE)\nval_ds   = val_ds.prefetch(AUTOTUNE)\ntest_ds  = test_ds.prefetch(AUTOTUNE)\n\n# -----------------------------\n# 3. Data augmentation pipeline\n# -----------------------------\ndata_augmentation = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.05),\n        layers.RandomZoom(0.1),\n        layers.RandomTranslation(0.05, 0.05),\n    ],\n    name=\"data_augmentation\",\n)\n\n# -----------------------------\n# 4. Build the base MobileNetV2\n# -----------------------------\nbase_model = MobileNetV2(\n    input_shape=IMG_SIZE + (3,),\n    include_top=False,\n    weights=\"imagenet\",\n)\n\n# Freeze base model for the warm-up phase\nbase_model.trainable = False\n\n# -----------------------------\n# 5. Build the full model\n# -----------------------------\ninputs = keras.Input(shape=IMG_SIZE + (3,), name=\"input_image\")\n\n# (a) Data augmentation\nx = data_augmentation(inputs)\n\n# (b) Rescale pixels to [0, 1]\nx = layers.Rescaling(1.0 / 255)(x)\n\n# (c) Pass through base MobileNetV2\nx = base_model(x, training=False)\n\n# (d) Pooling + classification head\nx = layers.GlobalAveragePooling2D(name=\"global_avg_pool\")(x)\nx = layers.Dropout(0.3, name=\"dropout\")(x)\noutputs = layers.Dense(num_classes, activation=\"softmax\", name=\"predictions\")(x)\n\nmodel = keras.Model(inputs, outputs, name=\"fruits_veggies_mobilenetv2\")\n\nmodel.summary()\n\n# -----------------------------\n# 6. Compile (warm-up training)\n# -----------------------------\ninitial_lr = 1e-3\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=initial_lr),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\ninitial_epochs = 5\n\nhistory_warmup = model.fit(\n    train_ds,\n    epochs=initial_epochs,\n    validation_data=val_ds,\n)\n\n# -----------------------------\n# 7. Fine-tune (unfreeze some layers)\n# -----------------------------\n# Unfreeze the top part of MobileNetV2 for fine-tuning\nbase_model.trainable = True\n\n# Optionally: freeze earlier layers, fine-tune only last N\nfine_tune_at = 100  # unfreeze from this layer onwards (tune as needed)\n\nfor i, layer in enumerate(base_model.layers):\n    if i < fine_tune_at:\n        layer.trainable = False\n    else:\n        layer.trainable = True\n\n# Recompile with a much lower LR\nfine_tune_lr = 1e-5\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=fine_tune_lr),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nfine_tune_epochs = 10\ntotal_epochs = initial_epochs + fine_tune_epochs\n\nhistory_finetune = model.fit(\n    train_ds,\n    epochs=total_epochs,\n    initial_epoch=history_warmup.epoch[-1] + 1,\n    validation_data=val_ds,\n)\n\n# -----------------------------\n# 8. Evaluate and save\n# -----------------------------\nloss, acc = model.evaluate(val_ds)\nprint(f\"Validation accuracy after fine-tuning: {acc:.4f}\")\n\nmodel.save(\"mobilenetv2_fruits_veggies_finetuned.h5\")\nprint(\"Model saved as mobilenetv2_fruits_veggies_finetuned.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T16:25:23.209042Z","iopub.execute_input":"2025-11-19T16:25:23.209686Z","iopub.status.idle":"2025-11-19T16:32:36.249682Z","shell.execute_reply.started":"2025-11-19T16:25:23.209662Z","shell.execute_reply":"2025-11-19T16:32:36.248998Z"}},"outputs":[{"name":"stdout","text":"Found 3115 files belonging to 36 classes.\nFound 351 files belonging to 36 classes.\nFound 359 files belonging to 36 classes.\nClasses: ['apple', 'banana', 'beetroot', 'bell pepper', 'cabbage', 'capsicum', 'carrot', 'cauliflower', 'chilli pepper', 'corn', 'cucumber', 'eggplant', 'garlic', 'ginger', 'grapes', 'jalepeno', 'kiwi', 'lemon', 'lettuce', 'mango', 'onion', 'orange', 'paprika', 'pear', 'peas', 'pineapple', 'pomegranate', 'potato', 'raddish', 'soy beans', 'spinach', 'sweetcorn', 'sweetpotato', 'tomato', 'turnip', 'watermelon']\nNum classes: 36\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"fruits_veggies_mobilenetv2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fruits_veggies_mobilenetv2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_image (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_avg_pool                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ predictions (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │        \u001b[38;5;34m46,116\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_avg_pool                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ predictions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">46,116</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,304,100\u001b[0m (8.79 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,304,100</span> (8.79 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,116\u001b[0m (180.14 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,116</span> (180.14 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1763569531.656811     122 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 312ms/step - accuracy: 0.2448 - loss: 2.9245 - val_accuracy: 0.8063 - val_loss: 0.7406\nEpoch 2/5\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 268ms/step - accuracy: 0.7032 - loss: 1.0229 - val_accuracy: 0.8746 - val_loss: 0.4781\nEpoch 3/5\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 261ms/step - accuracy: 0.7940 - loss: 0.7449 - val_accuracy: 0.9003 - val_loss: 0.3736\nEpoch 4/5\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 265ms/step - accuracy: 0.8272 - loss: 0.6013 - val_accuracy: 0.9088 - val_loss: 0.3260\nEpoch 5/5\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 268ms/step - accuracy: 0.8249 - loss: 0.5527 - val_accuracy: 0.9031 - val_loss: 0.2980\nEpoch 6/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 279ms/step - accuracy: 0.6339 - loss: 1.2347 - val_accuracy: 0.9174 - val_loss: 0.2757\nEpoch 7/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 269ms/step - accuracy: 0.7430 - loss: 0.8656 - val_accuracy: 0.9202 - val_loss: 0.2704\nEpoch 8/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 272ms/step - accuracy: 0.7807 - loss: 0.7338 - val_accuracy: 0.9202 - val_loss: 0.2626\nEpoch 9/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 273ms/step - accuracy: 0.8018 - loss: 0.6410 - val_accuracy: 0.9231 - val_loss: 0.2548\nEpoch 10/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 271ms/step - accuracy: 0.8228 - loss: 0.5938 - val_accuracy: 0.9288 - val_loss: 0.2484\nEpoch 11/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 274ms/step - accuracy: 0.8304 - loss: 0.5483 - val_accuracy: 0.9316 - val_loss: 0.2427\nEpoch 12/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 271ms/step - accuracy: 0.8470 - loss: 0.5285 - val_accuracy: 0.9316 - val_loss: 0.2375\nEpoch 13/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 278ms/step - accuracy: 0.8567 - loss: 0.4794 - val_accuracy: 0.9345 - val_loss: 0.2296\nEpoch 14/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 276ms/step - accuracy: 0.8536 - loss: 0.4631 - val_accuracy: 0.9345 - val_loss: 0.2216\nEpoch 15/15\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 274ms/step - accuracy: 0.8745 - loss: 0.4199 - val_accuracy: 0.9402 - val_loss: 0.2182\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 306ms/step - accuracy: 0.9107 - loss: 0.3188\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"Validation accuracy after fine-tuning: 0.9402\nModel saved as mobilenetv2_fruits_veggies_finetuned.h5\n","output_type":"stream"}],"execution_count":5}]}